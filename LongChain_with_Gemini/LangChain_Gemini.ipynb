{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPSjWWMVf1RCCdNKu6DhmT5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X0MZzVsLa4_X","executionInfo":{"status":"ok","timestamp":1726936735477,"user_tz":-300,"elapsed":12922,"user":{"displayName":"Kinzer Rahman","userId":"15838613920431360737"}},"outputId":"1cb9fe01-586a-4686-dbe5-9d01a45a82a7"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.9/399.9 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.2/290.2 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["%pip install -qU langchain-google-genai"]},{"cell_type":"code","source":["# When you install a new package, it’s often necessary to restart the kernel to refresh the environment so the package can be recognized and used. The snippet you provided automates this process by shutting down and restarting the kernel.\n","# Automatically restart kernel after installs so that your environment can access the new packages\n","import IPython\n","\n","app = IPython.Application.instance()\n","app.kernel.do_shutdown(True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qsLpt3mmc3mS","executionInfo":{"status":"ok","timestamp":1726936957443,"user_tz":-300,"elapsed":9,"user":{"displayName":"Kinzer Rahman","userId":"15838613920431360737"}},"outputId":"51a0a3b8-3f25-4351-8148-101d68c36d07"},"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'status': 'ok', 'restart': True}"]},"metadata":{},"execution_count":1}]},{"cell_type":"markdown","source":[],"metadata":{"id":"Mwc-UZsQdz34"}},{"cell_type":"markdown","source":["## Creating a JSON File for Authentication or Allowing Gemini API in Langchain\n","\n","If you're facing authentication issues when trying to access the Gemini API in Langchain, particularly when using environment variables to store your API key, follow these steps:\n","\n","### Step 1: Set Up Your Project in Google Cloud Console\n","* Open **Google Cloud Console**.\n","* At the top left corner, click on **Select a Project**. A new window will pop up.\n","* Select an existing project or create a new one (Free Version).\n","\n","### Step 2: Enable the Gemini API\n","* After selecting or creating a project, go to the **WELCOME** screen.\n","* Select **APIs and Services** from Quick Access.\n","* In the **APIs and Services** window, look for **Library** in the left sidebar and click on it.\n","* In the search box, type **Gemini API**. Two results will be shown:\n","  * **Gemini API**\n","  * **Gemini for Google Cloud**\n","* Select the first one and click on **Enable** to enable the Gemini API.\n","\n","### Step 3: Generate an API Key\n","* Return to the **APIs and Services** window.\n","* Select **Credentials** from the top-level menu.\n","* Click on **Create Credential**, then select **API Key** from the dropdown. An API key will be generated.\n","\n","### Step 4: Create a Service Account\n","* Click on **Google Cloud** at the top left corner to return to the Welcome page.\n","* From Quick Access, select **IAM and Admin**.\n","* In the left sidebar, select **Service Accounts** and click on **Create Service Account**.\n","* Fill in the required details and click on **Create and Continue**.\n","* Click on **Select Role**, search for **OWNER**, and select the role with full privileges.\n","* Click on **Continue**.\n","* Skip the remaining fields and click on **Done**.\n","\n","### Step 5: Generate and Download a JSON Key File\n","* In the **Service Accounts** window, click on the email of the service account you just created.\n","* Click on the newly generated email.\n","* At the top, below the key name where it's mentioned, click on **KEYS** in the taskbar above.\n","* In the **Keys** window, select **Add Key**.\n","* Click on **Create New Key**.\n","* Ensure **JSON** is selected.\n","* Click on **Create Key**. A pop-up will ask you to save the file locally.\n","* Save the JSON file and upload it to the root directory of your Colab environment.\n","* Right-click on the newly added file and select **Copy Path**. Replace the path in your script or notebook with this copied path.\n"],"metadata":{"id":"_nc_fi-JeFIj"}},{"cell_type":"code","source":["# GOOGLE_APPLICATION_CREDENTIALS environment variable, which is often required when using Google Cloud\n","# It ensures that the environment variable is set, and Google APIs can find the credentials they need to authenticate your requests.\n","import os\n","os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Json file.json\""],"metadata":{"id":"MzW-26x5ewNi","executionInfo":{"status":"ok","timestamp":1726938630144,"user_tz":-300,"elapsed":533,"user":{"displayName":"Kinzer Rahman","userId":"15838613920431360737"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["\n","from langchain_google_genai import ChatGoogleGenerativeAI\n","\n","llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n","ai_msg = llm.invoke(\"What is the capital of France?\")\n","print(ai_msg.content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AO3hYVpFf13n","executionInfo":{"status":"ok","timestamp":1726938656590,"user_tz":-300,"elapsed":8387,"user":{"displayName":"Kinzer Rahman","userId":"15838613920431360737"}},"outputId":"af6f9ef7-03ff-4364-817a-2bdaa19e680a"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["The capital of France is **Paris**. \n","\n"]}]},{"cell_type":"code","source":["# Define the conversation\n","message = [\n","    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n","    {\"role\": \"user\", \"content\": \"Which open source AI Model is best so far\"},\n","]\n","\n","# Invoke the language model to get a response\n","ai_msg = llm.invoke(message)\n","\n","# Output the response\n","print(ai_msg)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QHoK-xjxkUy6","executionInfo":{"status":"ok","timestamp":1726938931738,"user_tz":-300,"elapsed":3532,"user":{"displayName":"Kinzer Rahman","userId":"15838613920431360737"}},"outputId":"fc853379-5015-4ce0-f469-4810db24f163"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["content='It\\'s impossible to definitively say which open-source AI model is \"best\" as it depends heavily on your specific needs and use case.  Here\\'s a breakdown to help you decide:\\n\\n**Factors to Consider:**\\n\\n* **Task:** What do you want the AI to do? (e.g., text generation, image classification, translation, code generation)\\n* **Data:** What kind of data will you be feeding the model? (e.g., text, images, audio)\\n* **Resources:** How much computing power and memory do you have available?\\n* **Ease of Use:** How familiar are you with AI models and frameworks?\\n* **Specific Requirements:** Are there any specific features or capabilities you need?\\n\\n**Popular Open-Source AI Models:**\\n\\n**For Text-Based Tasks:**\\n\\n* **GPT-Neo:** A powerful language model similar to GPT-3, available in various sizes. \\n* **BLOOM:** A large language model trained on 46 languages, excelling in translation and multilingual tasks.\\n* **BigScience:** A community-driven project developing large language models with a focus on ethical and responsible AI.\\n* **BART:** A model specializing in text summarization, translation, and question answering.\\n* **T5:** A model designed for various NLP tasks, including text-to-text generation.\\n\\n**For Image-Based Tasks:**\\n\\n* **CLIP:** A model that connects images and text, enabling tasks like image captioning and visual question answering.\\n* **Stable Diffusion:** A powerful text-to-image generation model, capable of creating realistic and imaginative images.\\n* **DALL-E 2:** (Technically not open-source, but similar in functionality) A model that generates images from text descriptions.\\n\\n**For Other Tasks:**\\n\\n* **OpenAI Whisper:** An automatic speech recognition model for transcribing audio.\\n* **DeepFaceLab:** A tool for creating deepfakes (though use ethically!)\\n\\n**Choosing the Right Model:**\\n\\n* **Start with your needs:** Identify the specific task you want to accomplish.\\n* **Research models:** Explore the capabilities and limitations of different open-source models.\\n* **Consider resources:** Ensure you have the necessary computing power and memory.\\n* **Experiment:** Test out different models and find the one that performs best for your use case.\\n\\n**Remember:**\\n\\n* **Ethical considerations:**  Be mindful of the potential risks and ethical implications of using AI models.\\n* **Community support:** Choose a model with a strong community and active development.\\n* **Documentation:** Ensure the model has comprehensive documentation and tutorials.\\n\\n**In conclusion:** There is no single \"best\" open-source AI model. Choose the one that best suits your specific needs and resources.\\n' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]} id='run-cdb577af-ee75-44aa-b402-d54998cd55eb-0' usage_metadata={'input_tokens': 16, 'output_tokens': 576, 'total_tokens': 592}\n"]}]},{"cell_type":"markdown","source":["## Importing Modules\n","\n","- **`textwrap`**: This module is used to manipulate text by wrapping and indenting it.\n","- **`Markdown` from `IPython.display`**: This is used to display text in Markdown format within Jupyter Notebooks.\n","\n","## Function Definition\n","\n","- **`def to_markdown(text) -> Markdown:`**: This defines a function called `to_markdown` that takes a string (`text`) as input and is expected to return a `Markdown` object.\n","\n","## Text Replacement\n","\n","- **`text: str = text.replace(\"•\", \" *\")`**: This line replaces any occurrence of the bullet point character (•) with a Markdown-style unordered list asterisk (`*`). It also includes two spaces before the asterisk, which is a common convention in Markdown to create nested lists or proper indentation.\n","\n","## Text Indentation\n","\n","- **`textwrap.indent(text, \"> \", predicate=lambda _: True)`**: This function indents every line of the text with the specified string (`\"> \"`), which is used to format the text as a blockquote in Markdown. The `predicate=lambda _: True` argument ensures that every line of the text is indented.\n","\n","## Returning Markdown Object\n","\n","- **`return Markdown(...)`**: This returns a `Markdown` object containing the indented and modified text, making it ready for rendering in a Jupyter Notebook or any environment that supports Markdown.\n"],"metadata":{"id":"9AGsfOCbmynS"}},{"cell_type":"code","source":["\n","import textwrap\n","from IPython.display import Markdown\n","\n","\n","def to_markdown(text)->Markdown:\n","    text : str = text.replace(\"•\", \"  *\")\n","    return Markdown(textwrap.indent(text, \"> \", predicate=lambda _: True))"],"metadata":{"id":"BOURgGbyl3VY","executionInfo":{"status":"ok","timestamp":1726939103861,"user_tz":-300,"elapsed":541,"user":{"displayName":"Kinzer Rahman","userId":"15838613920431360737"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# AI (which is stored in ai_msg.content), format it using the to_markdown function, and convert it into a Markdown object.\n","to_markdown(ai_msg.content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":684},"id":"6ZE0hfmGnbf9","executionInfo":{"status":"ok","timestamp":1726939916207,"user_tz":-300,"elapsed":896,"user":{"displayName":"Kinzer Rahman","userId":"15838613920431360737"}},"outputId":"d29489bd-c8ad-4b52-a778-b70b911c2df6"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"> It's impossible to definitively say which open-source AI model is \"best\" as it depends heavily on your specific needs and use case.  Here's a breakdown to help you decide:\n> \n> **Factors to Consider:**\n> \n> * **Task:** What do you want the AI to do? (e.g., text generation, image classification, translation, code generation)\n> * **Data:** What kind of data will you be feeding the model? (e.g., text, images, audio)\n> * **Resources:** How much computing power and memory do you have available?\n> * **Ease of Use:** How familiar are you with AI models and frameworks?\n> * **Specific Requirements:** Are there any specific features or capabilities you need?\n> \n> **Popular Open-Source AI Models:**\n> \n> **For Text-Based Tasks:**\n> \n> * **GPT-Neo:** A powerful language model similar to GPT-3, available in various sizes. \n> * **BLOOM:** A large language model trained on 46 languages, excelling in translation and multilingual tasks.\n> * **BigScience:** A community-driven project developing large language models with a focus on ethical and responsible AI.\n> * **BART:** A model specializing in text summarization, translation, and question answering.\n> * **T5:** A model designed for various NLP tasks, including text-to-text generation.\n> \n> **For Image-Based Tasks:**\n> \n> * **CLIP:** A model that connects images and text, enabling tasks like image captioning and visual question answering.\n> * **Stable Diffusion:** A powerful text-to-image generation model, capable of creating realistic and imaginative images.\n> * **DALL-E 2:** (Technically not open-source, but similar in functionality) A model that generates images from text descriptions.\n> \n> **For Other Tasks:**\n> \n> * **OpenAI Whisper:** An automatic speech recognition model for transcribing audio.\n> * **DeepFaceLab:** A tool for creating deepfakes (though use ethically!)\n> \n> **Choosing the Right Model:**\n> \n> * **Start with your needs:** Identify the specific task you want to accomplish.\n> * **Research models:** Explore the capabilities and limitations of different open-source models.\n> * **Consider resources:** Ensure you have the necessary computing power and memory.\n> * **Experiment:** Test out different models and find the one that performs best for your use case.\n> \n> **Remember:**\n> \n> * **Ethical considerations:**  Be mindful of the potential risks and ethical implications of using AI models.\n> * **Community support:** Choose a model with a strong community and active development.\n> * **Documentation:** Ensure the model has comprehensive documentation and tutorials.\n> \n> **In conclusion:** There is no single \"best\" open-source AI model. Choose the one that best suits your specific needs and resources.\n"},"metadata":{},"execution_count":13}]}]}